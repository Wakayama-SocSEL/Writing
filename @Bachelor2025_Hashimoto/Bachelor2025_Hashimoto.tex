\documentclass[11pt]{jreport}
\usepackage{wuse_thesis}
\usepackage{indentfirst}
\usepackage{url}	% \url{}コマンド用．URLを表示する際に便利
%\usepackage{graphicx}  % ←graphicx.styを用いてEPSを取り込む場合有効にする
			% 他のパッケージ・スタイルを使う場合には適宜追加
\usepackage{latexsym}
\usepackage[dvipdfmx]{graphicx,xcolor}
\usepackage{tabularx}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{amsmath}


\newcommand{\todo}[1]{\colorbox{yellow}{{\bf TODO}:}{\color{red} {\textbf{[#1]}}}}
\newcommand{\memo}[1]{\colorbox{magenta}{\textbf{MEMO}}{\color{red}\textbf{[#1]}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% 主に表紙を作成するための情報
%%

%%  タイトル(修論の場合は英語表記も指定)
\title{コードレビューにおける\\
長期貢献者予測に向けた学習期間の検討}
%\etitle{Test\\Test\\Test}

%%  著者名(修論の場合は英語表記も指定)
\author{橋本 一輝}
%\eauthor{Akinori Ihara}

%% 卒業論文・修士論文(以下のどちらかを選択)
\bachelar	% 卒業論文(4年生用)
%\master  	% 修士論文(M2用)

%%  学科・クラスタ
\department{システム工}
%\department{デザイン情報}
%\department{デザイン科学}

%%  学生番号
\studentid{60276189}

%%  卒業年度
\gyear{2025}		% 提出年が2022年なら，2021年度

%%  論文提出日
\date{2026年2月10日}	% 修士の場合は月(2021年2月)までとし，英語表記も指定
%\edate{February 2021}	% 修士の場合，こちら(英語表記)も有効化

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

%%
%%  概要
%%
\begin{abstract}
オープンソースソフトウェアプロジェクトでは，長期的に貢献する開発者（長期貢献者）が継続的な開発・保守において重要な役割を担っている．プロジェクトに参加してから1年後まで活動する長期貢献者を早期に特定するためには，少なくともプロジェクト参加後1ヶ月間の貢献の観察を要する．しかし，その期間の貢献が2ヶ月以降1年以内の活動に寄与しているかは明らかでない．
本研究は，開発者がコードレビュー依頼を継続的に引き受けるか否かを予測するする手法を提案する．具体的には，逆強化学習を用いてレビュアーの貢献を学習し，継続的なコードレビュー依頼の受け入れ可否を決定する報酬関数を推定するモデルを構築する過程で，高い精度で予測可能な学習期間を検討する．ケーススタディとして，OpenStack/Novaプロジェクトを対象とし，報酬関数に基づき予測を行なった結果，提案モデルはAUC-ROCが0.820で予測可能であり，予測に寄与する特徴量が，短期では活動量が重視され，長期では質的な要因が重視されるようになることが明らかになった．また，学習期間に応じて高い適合率を得られる予測期間は異なることが明らかになった．
\end{abstract}

%%  目次
\tableofcontents

%%  図目次 (図目次をいれたければ以下のコメントをはずす)
%\listoffigures

%%  表目次 (表目次をいれたければ以下のコメントをはずす)
%\listoftables

\newpage
\pagenumbering{arabic}	% 以降のページ番号を算用数字に

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%%  本文はここから
%%

%1
\chapter{はじめに}
オープンソースソフトウェア (OSS) は，地理的に分散した開発者によって実装，保守されているが，多くの開発者は，OSS開発組織から金銭的な利益を得ることなく，個人の知的好奇心や開発技術の学習などを動機として貢献している\cite{motivation}.

Raymondらは，OSS開発の成功例としてLinuxを対象とした調査し, 不特定多数の開発者が自発的に参加する開発形態をバザール方式と名付けた\cite{bazaar}．この開発モデルは，多様な専門知識を有する開発者の参加を促進する一方で，開発者の流動性が極めて高い\cite{movement}．
また，開発者の動機が多様なため，時間的制約や興味の変化によって数回の貢献後に活動を停止する開発者が多い\cite{OTC}．こうした開発者の離脱は，OSSプロジェクトの失敗の一要因になっている\cite{failed}.



OSSを長期的に運用，保守しているプロジェクトでは，長期的に貢献する長期的貢献者 (LTC) が重要な役割を担っている\cite{related1}\cite{successful}．

LTCが，ソフトウェアの大部分を実装しているプロジェクトも存在する\cite{LTC}．
また，実装に限らず，コードレビューや，新規参加した開発者のメンターなどの役割を担っている．また，一部のLTCは，リポジトリの主要なブランチにコミットする権限が与えられプロジェクトの継続的な運用を支えている\cite{LTC}．このような経験が豊富なLTCは，流動性の高い組織構造を有するOSSプロジェクトに少数しか存在しないため，LTCに作業負担がかかることが少なくない\cite{related2}．LTCへの負担軽減のためにも，継続的に貢献する開発者の参加が喫緊の課題である．




従来研究では，将来的にLTCとして長期間にわたってプロジェクトに参加する開発者を早期に特定する手法が提案されている\cite{related1}\cite{LTC}．しかし，LTCが担うタスクの1つであるコードレビュー作業への継続的な貢献に着目した研究は十分に進められていない．

本研究は，開発者が作業依頼を受け入れるか，あるいは依頼に反応しないかを明確に判断することができるコードレビューを題材に，将来のコードレビュー依頼に対するレビュアーの貢献行動可否を，開発者の継続的な貢献と捉えたLTC候補者予測モデルを提案する．ここで開発者が依頼を受け入れるかどうかは開発者の興味や状況により左右される．受け入れることは興味や貢献する意思があることを示唆し，反応がない場合には継続的に貢献する意思がないことを示唆する．
\begin{itemize}
    \item RQ1:逆強化学習に基づく提案モデルは，コードレビューにおけるタスク受け入れをどの程度予測できるか？
    \item RQ2:学習方法や予測期間の長さに応じて，予測モデルの精度はどのように変化するか?
    \item RQ3:推定された報酬関数において，長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か?
\end{itemize}


続く\ref{sec:related}章では，本研究の関連研究を紹介し，本研究の位置付けを明確にし，\ref{sec:ml}章では，本研究で用いる逆強化学習とそれに関連する強化学習を紹介する．\ref{sec:model}章では，本研究の提案手法を述べ，\ref{sec:casestudy}章でケーススタディの結果を述べRQに回答する．\ref{sec:discussion}で本研究の結果を考察し，\ref{sec:conclusion}章で本研究をまとめる．




%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%3
\chapter{持続可能なOSS開発に向けて}\label{sec:related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{OSS開発の現状}
オープンソースソフトウェア(OSS: Open Source Software)では，ソースコードが公開され，ライセンスの条件下において，誰でも自由に閲覧・修正・再配布が可能なソフトウェアであり，世界中に地理的に分散した多くのボランティア開発者の貢献により開発が行われている．
一般的な，OSS開発のフローは次のように進行する．まず，ユーザや開発者が，発見したバグや機能追加の要望などをissueやチケットとして報告する．そして，開発者はコードを修正・追加することでissueの対応を行い，その変更内容をパッチやプルリクエストとしてプロジェクトに提出する．提出された変更は，プロジェクトメンバーによるコードレビューを受ける．膨大な変更提案を効率的に処理するために，誰がコードレビューを担当するかを明確にするアサインという仕組みが取られている．具体的には，変更されたファイルの管理者（コードオーナー）がシステムによって自動的に割り当てられたり，変更提案者やプロジェクトの管理者が適切なレビュアーを指名したりすることにより，レビュータスクが振り分けられる．レビュアーとの議論を経て，最終的に変更が承認された場合には，変更がリポジトリに統合される．
しかし，このOSS開発プロセスでは，開発者の多くがボランティアであるために，アサインというタスク割り当てが存在しても，強制力がないために，アサインされても反応がなく放置されるケースが存在することである．さらに，開発者は個人の事情や興味の変化などによりいつでもプロジェクトから離脱することができ，離脱や貢献といった動機は明確でないため，労働力が安定しないことや労働力を見積もることが困難であるといった課題がある．
こうした環境の中で，OSSの持続可能な開発において重要であるのは，プロジェクトに長期的に貢献する開発者であるLTC(長期貢献者)である．しかし，OSSプロジェクトにおいてLTCとなり得る開発者は少数であり，特定の貢献者に負担が集中し，LTCの疲弊や離脱を招くリスクがある．そのため，多様な動機を持つ開発者の中から，継続的に貢献するLTCなり得る開発者を特定し，適切なサポートを行うことが重要となる．




\section{持続可能なOSS開発に向けた関連研究}
\subsection{OSSへ継続的に貢献する開発者の研究}

従来研究では，開発者が特定のプロジェクトで活動を継続する動機を調査している．Wuらによる定性的分析において，OSS開発者は利他的な動機（例：貢献による名誉や実績）や人的資本の向上に関連する経済的動機（例：専門知識を有する開発者との協調）をもってOSSプロジェクトへの参加を継続していることを明らかにしている\cite{motivation2}．

その他，多くの研究者がOSSプロジェクトにおける新規参加者の定着に影響を与える要因を調査している\cite{LTC}\cite{bib1}\cite{bib2}\cite{abadon}．

Birdらは，開発者がOSSプロジェクトに貢献する期間に影響を与える要因を理解するためのハザード率モデル（hazard-rate model）を提案している\cite{bib1}.

ケーススタディとして，Apache，Python，PostgreSQLプロジェクトでは，特定のプロジェクトで約1年間活動した開発者をリポジトリの主要なブランチにコミットする権限を与えていることを示している．特に，ApacheおよびPythonプロジェクトでは，継続的にパッチを作成している開発者に権限を与えていることを明らかにしている．このように，OSSプロジェクトは，LTC候補者に継続的な貢献を期待していることが確認できる．


\subsection{長期貢献者候補者の研究}

従来研究では，LTC候補者を予測する手法が提案されている\cite{related1}\cite{LTC}．

Zhouらは，OSSプロジェクトに新規に参加した開発者の中からLTC候補者を特定するモデルを開発している\cite{LTC}．当該研究において，LTC候補者に分類される開発者は，技術的能力，意欲，および環境に依存することを明らかにしている．特に，LTC候補者を特定するためには，プロジェクト参加後の1ヶ月間の貢献を観察し，「参加後１ヶ月間の活動量」などのメトリクスを用いることで，0.80程度のAUCスコアでLTCを特定することを実現している．

ELuriらは，OSSプロジェクトに新規に貢献した開発者の特徴量，および貢献したリポジトリの特徴量に基づき，開発者が3年後に貢献しているか否かを予測するモデルを開発している\cite{related1}．提案モデルは，従来手法の予測精度に比べて非常に高く，AUCが0.913という結果を得ている．当該研究の比較対象とされたBaoらの研究も同様に，T年後 (T=1 ,2, 3) に開発者が貢献しているか否かを，プロジェクト参加直後1ヶ月間の貢献から予測モデルを構築している．これらの研究は，OSSの開発記録であるコミットやイシューへの対応など，プロジェクトでの全般的な活動を貢献とみなし，将来の単一時点での予測に留まっている．


\subsection{開発者の離職予測に関する研究}
Baoらは，商用企業を対象とし，プロジェクトに参加直後の6ヶ月間の月次レポートに基づき，1年後に離職しているか否かを予測するモデルを提案している\cite{turnover}．予測に寄与する特徴量には，月次レポートのタスク報告内容，労働時間の標準偏差，プロジェクト開発者での労働時間の標準偏差であった．この研究も同様に，月次レポートに基づく活動でありプロジェクトでの全般的な活動を貢献とみなしている．また，予測に関しても単一の時点である．

\section{本研究の位置付け}
%2.1及び2.2の
従来研究では，コミットや月次レポートなどのプロジェクトへの全ての貢献量に基づき，特定の期間以降の開発者の自主的な貢献の有無を予測する手法を提案している．OSS開発では，自主的に貢献する活動に限らず，開発者間の協調作業として作業依頼に基づく活動も少なくない．本研究では，他の開発者からの依頼，特にコードレビュー依頼を受け入れるか否かを予測し，開発者の継続的な貢献と捉えるLTC候補者予測モデルを提案する．具体的には，開発者のOSSの貢献パターンに応じて，0$\sim$3ヶ月後，3$\sim$6ヶ月後，6$\sim$9ヶ月後，9$\sim$12ヶ月後にそれぞれ貢献の有無を予測する．

OSS開発者の中には，一時的にプロジェクトでの活動を休止することが少なくない．そのため，従来研究では開発者の継続的な活動期間を計測することが困難である．本研究で提案するモデルは開発者の過去の時系列データを用いるため，開発者の個人的な都合による一時的に休止を直接的に理解することは困難であるが，特に作業がないために活動を休止しているレビュアーが作業依頼を受け入れるか否かを予測できるモデルが構築できると考える．

% 単一時点での予測を行っており，LTCや開発者の定着性を予測する上で重要な知見を提供している．
% ここで，本研究ではプロジェクトでの全般的な活動ではなく，コードレビュー作業というより専門的な活動に焦点を当て分析する．
% また，予測においてもT年後という単一時点の予測から0-3ヶ月後，3-6ヶ月後，6-9ヶ月といった連続的な区間における貢献の有無を予測する．これにより，従来の研究では捉えることができなかった，貢献者が「いつ」活動し，「いつ」離脱するのかといった動的なパターンを捉えることを可能にすることを目的とする．具体的には．また，一度活動を休止した後に再び貢献するといったパターン（例:0-3ヶ月後は活動，3-6ヶ月後は休止,6-9ヶ月後に活動）を特定できる可能性がある．これにより，プロジェクトの管理者が，開発者が「いつ」離脱する可能性が高くなるか，あるいは復帰する可能性があるかを予測することができる．

%4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{LTC予測に向けた強化学習と逆強化学習}\label{sec:ml}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

本研究では，時系列を重視し開発者の行動を捉えるために，逆強化学習を用いることでLTC候補者予測モデルを構築する.

\section{強化学習}
強化学習 (RL: Reinforcement Learning) は，エージェントと環境が相互作用を通じて，累積報酬を最大化するような行動の方策を学習する機械学習の一分野である．表\ref{tab:RL}に示す基本的な構成要素から成る．
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{./Hashimoto_fig/RL.pdf}
    \caption{強化学習のイメージ図.}
    \label{fig:RL}
\end{figure}




\begin{table}[h]
\centering 
\caption{強化学習の主な構成要素}
\label{tab:RL} 
\begingroup 
\setlength{\tabcolsep}{4pt} 
\footnotesize 
    \begin{tabularx}{\columnwidth}{lXX} 
    \toprule
    用語 & 説明 & OSS適応例 \\
    \midrule
    エージェント & 学習・意思決定を行う主体 & レビュアー \\
    環境 & エージェントと相互作用する対象 & OSSプロジェクト \\
    状態 & 環境の現在状況 & レビュアーの活動履歴など \\
    行動 & エージェントが取る選択肢 & レビュー依頼を引き受けるかどうか \\
    報酬 & 行動に応じて与えられるスカラー量 & 承諾なら高報酬 \\
    方策 & エージェントの行動を決定する & レビューを引き受ける際の判断基準 \\
    \bottomrule
    \end{tabularx}
\endgroup
\end{table}

強化学習において，報酬関数$R(s, a)$は最も重要な要素の一つであり，エージェントにとって何が望ましい行動であるかを示す指標となる．しかし，多くの実世界の問題では，多様な要因が複雑に作用しているため，事前に報酬関数を設計することは困難であり，不適切な報酬関数はエージェントを意図しない行動に導くことになる．

\section{逆強化学習}

強化学習において事前に報酬関数を設計することが困難である課題を解決するため，専門家の行動から，その行動を説明する報酬関数を逆算するアプローチとして逆強化学習が用いられる\cite{IRL}．既知の報酬関数から最適な方策を学習する強化学習に対して，逆強化学習は既知の専門家の軌跡から，その行動を説明する報酬関数を推定する．専門家の軌跡は，状態と行動を対として時系列に並べたものであり，専門家はある報酬関数（目的）を最大化するよう行動していると仮定し，その報酬関数を推定する．

従来の教師あり学習でのLTC予測では，単一時点でのスナップショットから予測を行うモデルを作成し，予測を行っていたため，活動頻度の変化などといった，活動のパターンの変化を捉えることが難しい．しかし，逆強化学習は時系列の活動を軌跡として扱い，各状態と行動から報酬関数を推定する．そのため，高負荷なタスクが重なる，レビューの応答時間が伸びるといった，従来では捉えることが難しかった行動パターンをより正確に捉えることができる，

\section{本研究における活用方法}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%5
\chapter{長期貢献者予測モデルの作成}
\label{sec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{データ収集・前処理}
本研究では，Gerrit REST APIを用いて，GerritからOpenStackプロジェクトのデータを取得する．

\begin{table}[h]
    \centering
    \label{table:dataList}
    \caption{収集データ}
    \begin{tabularx}{\columnwidth}{@{}lX@{}} 
        \toprule
        \textbf{項目} & \textbf{内容} \\
        \midrule
        \texttt{レビュー依頼} & 変更ID,プロジェクト名，作成日時，ステータス，コード変更量（追加/削除）\\ 
        \texttt{レビュアー情報} & メールアドレス，名前，アサイン日時 \\
        \texttt{レビュー活動} & レビューコメント内容，投票スコア，応答時刻 \\
        \texttt{活動統計情報} & レビュー回数，レビュー応答率，プロジェクト在籍期間 \\
        \bottomrule
    \end{tabularx}
\end{table}

収集したデータから自動化されたbotアカウント(CI/CD等)と判断されるものをパターンマッチにより除外する．具体的には，レビュアーのメールアドレスに，bot，cl，jenkins，zuul等のパターンを含むアカウントの除外を行なった．全てのデータをレビュー依頼時刻でソートし，時系列順に整列する．逆強化学習による継続予測では，訓練データと予測対象は時間的に分離されているのため，基準日を設けることで次のように分割する．
\begin{itemize}
    \item 学習期間：\\ 
    スナップショットより過去のnヶ月間の活動履歴を使用し，各ステップ時から見て過去の活動履歴のみを使用する．
    \item 予測期間：\\
    スナップショットより未来のn-mヶ月後の間で活動があるかどうかを予測する．
\end{itemize}



\section{学習指標の作成}\memo{順番変更}
\begin{figure}[h]
    \centering
    \includegraphics[width = 1.0\textwidth]{./Hashimoto_fig/label.pdf}
    \caption{指定期間が0-3mであった時の例}
    \label{fig:label}
\end{figure}


本研究では，LTC候補者予測モデルを構築するために，逆強化学習を用いて報酬関数を推定する．ここで用いる逆強化学習では，コードレビュー依頼時点から指定期間内にレビュー依頼の受け入れが生じた場合をTrue，生じなかった場合をFalseとする二値変数として表現し，これを学習の入力データとする．


% \subsubsection{ラベルの基準点の設定}
% レビュアーの活動ごとにラベルを付与すると，ラベルの数が膨大になり，学習時間の増加や，細かすぎるラベル付により類似した状態での学習が増加するため，過学習になる恐れがある．そのため，本研究では各月の末日を基準点とすることで，その月内の全活動に同一のラベルを付与する，これにより月単位での継続パターンを学習し，より一般化された予測モデルの構築を目指す．

\subsection{基準点の設定}
レビュアーの活動ごとに評価値を割り当てると，その数が膨大になり，学習時間の増加や，類似する状態に基づく学習が増加することで，過学習を引き起こすおそれがある．そこで，本研究では各月の末日を基準点とし，その月内の全活動に同一の評価値を割り当てる．本手法により，月単位での継続的な活動パターンを学習し，より詳細な粒度で予測モデルの構築を目指す．
\memo{内容変更}

\subsection{レビュー承諾の可否を判定}
学習指標の作成は具体的には以下の手順で行う．
% 実際のラベル付は次の通りに行う．
\begin{enumerate}
    \item 活動日から活動月を特定する
    \begin{itemize}
        \item 例:2022-01-05 $\rightarrow$ 2022年1月
    \end{itemize}
    \item その月の最終日を基準点とする
    \begin{itemize}
        \item 例：2022-01-31
    \end{itemize}
    \item 基準点から指定期間の間で将来の活動があるか調べる（例：指定期間が0-3mの場合）
    \begin{itemize}
        \item 例：2022-01-31から2022-04-31に活動があるか（基準日から指定期間）
    \end{itemize}
    \item レビュー依頼を受け入れるか否かを判定
    \begin{itemize}
        \item レビュー承諾 $\rightarrow$ True, レビュー拒否 $\rightarrow$ False
    \end{itemize}
        \item その月の全活動に同じラベルを付与する（4）でTrueの場合
    \begin{itemize}
        \item 1月の全活動にTrueラベルを付与
    \end{itemize}
\end{enumerate}

% \subsubsection{ラベルの学習時の役割}
% このラベルは，逆強化学習の際に単なる過去の事実としてではなく，その時点での状態が，将来の継続とどう関連しているかを学習するための教師信号として機能する．具体的には，Trueのラベルからは，継続したレビュアーの活動から継続を促進する要因を学習し，Falseのラベルからは離脱したレビュアーのパターンから離脱のシグナルを学習することができる．

\subsection{学習指標の役割}
本研究で用いる指標値は，逆強化学習において単なる過去の事実を示すものではなく，レビュー依頼時の状態が将来の継続にどのように関係するかを学習するための信号として機能する．具体的には，評価値がTrueの指標値からはレビュアーが貢献を継続するする要因を学習し，評価値がFalseの指標値からはレビュアーが依頼に反応しない行動パターンを示す特徴を学習する．


\section{状態・行動特徴量の生成}
本研究では，逆強化学習を行うにあたり，10個の状態特徴量（表3）と，4個の行動特徴量（表4）を使用する．

\begin{table}[h]
    \centering
    \label{table:stateList}
    \caption{状態特徴量（10次元）}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{特徴量名} & \textbf{定義} \\
        \midrule
        \texttt{経験日数} & 初回活動から現在までの日数 \\
        \texttt{総コミット数} & これまでの総コミット数 \\
        \texttt{総レビュー数} & これまでのレビュー数 \\
        \texttt{最近の活動頻度} & 1日あたりの平均活動量 \\
        \texttt{平均活動間隔} &  タイムスタンプ間の平均日数 \\
        \texttt{活動トレンド} & 月活動比が増加したかどうか \\
        \texttt{協力スコア} & 共同レビュー回数 \\ 
        \texttt{コード品質スコア} & test/doc/refactor/fixの頻度 \\
        \texttt{最近の受諾率} & 直近10件のレビュー依頼受諾数 \\
        \texttt{レビュー負荷} & 未完了レビュー数 \\
        \bottomrule
    \end{tabular}
\end{table}

\vspace{2mm}

\begin{table}[h]
    \centering
    \label{table:actionList}
    \caption{行動特徴量（4次元）}
        \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{特徴量名} & \textbf{定義} \\
        \midrule
        \texttt{強度} & レビューするファイル数 \\
        \texttt{協力度} & その人をレビューした回数 \\
        \texttt{応答速度} & レビューリクエストから応答までの日数 \\
        \texttt{レビュー規模} & 1回のレビューで変更された総行数 \\
        \bottomrule
    \end{tabular}
\end{table}





\section{モデル構造と学習フロー}本研究で学習に用いるニューラルネットワークは，エンコーダ，LSTM，デコーダから構成される．\memo{順序入れ替え}
\subsection{エンコーダ}
レビュアーの各活動を表す状態 (State) および行動 (Action) の2つの特徴量ベクトルを，それぞれ独立したエンコーダ（多層パーセプトロン)に入力する．これらのエンコーダは元のベクトルよりもより表現力のある高次元のベクトルに変換する役割を持つ．
\subsection{LSTM}
各時点でエンコードされた状態ベクトルと行動ベクトルを連結し，時系列プロセッサであるLSTM (Long Short-Term Memory) に入力する．LSTMは活動履歴を記憶し，時間的な依存関係を理解するため，単一の活動だけでなく，活動量が徐々に低下する動的なパターンも把握できる．

\subsection{デコーダ}
LSTMによりレビュアーの全活動が処理された後，LSTMで得られた時系列情報（特徴ベクトル）をもとに多層パーセプトロンで最終的な予測値を算出する．ここではLSTMで処理されたレビュアーの全活動履歴を考慮した情報をデーコードし，シグモイド関数により最終的な継続確率$P$を算出する.

\subsection{報酬関数}
逆強化学習では，専門家の行動を説明する報酬関数を学習する．本研究ではレビュアーがレビュー依頼を受け入れるか，あるいは反応しないかを指す．具体的には，レビュアーが状態$s$において，レビュー依頼を受け入れる，または反応しないという行動$a$を選択する価値を表す報酬関数$R(s, a; \theta)$を，ニューラルネットワークのパラメータ$\theta$として学習する．学習において，継続してレビュー依頼を受け入れるレビュアーの活動軌跡に対しては高い累積報酬を与え，依頼に反応しないレビュアーの活動軌跡に対しては低い累積報酬を与えるように，報酬関数$R(s, a; \theta)$を調整する．

継続確率$P$はシグモイド関数$\sigma$を通して，軌跡の累積報酬に関連付けられる．累積報酬が高いほど継続確率$P$は高くなる．学習時には4.2節で定義した学習指標を用い，予測確率Pと評価値の誤差を計算するために損失関数を用いる．この誤差が最小となるように，最適化アルゴリズム (Adam Optimizer) で反復的に学習を行うことにより，継続して貢献するレビュアーの行動パターンを高く評価し，離脱者のパターンを低く評価する報酬関数を作成する．

% 逆強化学習では専門家の行動を説明する報酬関数を学習する．具体的には，状態sで行動aを選択することの価値を表す報酬関数R(s,a；θ)を，ニューラルネットワーぅのパラメータθとして学習する．本研究での専門家は継続したレビュアーを指し，非専門家は離脱したレビュアーを指す．学習においては，専門家（継続者）の活動軌跡に対しては高い累積報酬を与え，非専門家（離脱者）の活動軌跡に対しては低い累積報酬を与えるように，報酬関数R(s,a;θ)を調整する．

\subsection{損失関数による報酬関数の最適化}


% \subsection{損失関数}
% 貢献者の予測というタスクにおいて，約８割の開発者が長期貢献者になる前に，離脱してしまうということが知られている[todo]，そのため，正例と負例の間で学習データ数に不均衡が生じる．この不均衡問題を対処するために，本研究では，Focal Lossを使用する．

従来研究では多くの開発者が長期貢献者と呼ばれるまでに離脱することが知られている\cite{OTC}.そのため，LTC候補者予測のデータセットは正例（LTC候補者）と負例（LTC候補者以外の開発者）の間で学習データ数に不均衡が生じる．この不均衡問題を対処するために，本研究では，Focal Lossを使用する．

% \subsection{損失関数の定義}
Focal Lossは，二値クロスエントロピー (BCE: Binary Cross-Entropy) を拡張した損失関数の一種である．まず，二値クロスエントロピーは，二値分類のモデルの予測確率と正解ラベルの差を評価する損失関数であり，正解に近い予測ほど損失が小さく，誤った予測ほど損失が大きくなる．二値クロスエントロピーは次の式で示せる．
\[\textbf{二値クロスエントロピー：Binary Cross-Entropy (BCE)}\]
\[
L_{\mathrm{BCE}} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i\log(p_i) + (1 - y_i)\log(1 - p_i)\right]
\]
ここで，$N$はデータセットのサンプル数，$y_i \in \{0, 1\}$は$y_i$の正解ラベル，モデルが予測したクラスの確率は$p_i \in [0, 1]$を示す．

\[p_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}\]はシグモイド関数を表す．しかし，二値クロスエントロピーは，クラス不均衡が生じた際に，容易に分類できるサンプルばかりが学習され，難しいサンプルの学習が十分に進まない課題が存在する．Focal Lossはこの問題を解決するように設計されている．具体的には，正しく予測できる容易なサンプルの損失の重みを小さくし，誤分類されやすい難しいサンプルに学習の焦点が当たるように調整する．Focal Lossは次の式で示せる．

% Focal Loss(FL)は，二値クロスエントロピー（BCE）において，クラスの不均衡が生じた際に，容易に分類できる学習ばかりが進み，分類が困難なサンプルの学習ができない問題を解決するために設計された．具体的には，分類が容易である（正しく予測することができる）サンプルの損失の重みを小さくし，難しいサンプルに焦点が当たるように設計されている
% \[\textbf{二値クロスエントロピー：Binary Cross-Entropy (BCE)}\]
% \[
% L_{\mathrm{BCE}} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i\log(p_i) + (1 - y_i)\log(1 - p_i)\right]
% \]
% ここで，$N$はデータセットのサンプル数，$y_i \in \{0, 1\}$は$y_i$の正解ラベル，モデルが予測したクラスの確率は$p_i \in [0, 1]$を示す．

% \[p_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}\]はシグモイド関数を表す．


\[\textbf{Focal Loss (FL)}\]
\begin{align}
L_{\mathrm{FL}} = - \frac{1}{N} \sum_{i=1}^{N} \Bigl( & \alpha (1 - p_i)^{\gamma} y_i \log(p_i) \nonumber \\
& + (1 - \alpha) p_i^{\gamma} (1 - y_i) \log(1 - p_i) \Bigr) \nonumber 
\end{align}


ここで，
\(\alpha \in [0,1]\) はクラス不均衡を補正する重みパラメータを示し，
\(\gamma \ge 0\) は難しいサンプルを強調する焦点パラメータである．  
\(\gamma = 0\) のとき，Focal Loss は BCE に一致する．


% \subsubsection{報酬関数}
\begin{figure*}[t]
    \centering
    \includegraphics[width = 1.0\textwidth]{./Hashimoto_fig/prediction.pdf}
    \caption{学習から予測のフロー}
    \label{fig:flow}
\end{figure*}
















% \subsection{Focal Lossによる報酬関数の最適化}




\section{予測・評価}
% 本研究では，学習時に予測時点以降の情報を使用することを防ぐために，学習期間と予測期間を分離するだけでなく，特徴量計算に使用できるデータの最大時点 (Max-date) を設けている．
% 具体的には，各時点においてはその時点以前のデータから特徴量を計算し，その月末の時点で継続ラベルを付与するが，Max-date以降のデータはラベル生成の際のみ使用する．これにより，モデルが将来の情報を事前に学習することを防ぐことができる．
% ここで，最適化された報酬関数を用いて，特徴量に対する報酬を算出する．この報酬値にシグモイド関数を適応することで0\textasciitilde1の継続確率に変換することで予測を行う．

本研究では，学習時に予測時点以降の情報を使用することを防ぐため，学習期間と予測期間を分離するだけでなく，特徴量計算に使用可能なデータの最大時点（Max-date）を設定している．具体的には，各時点においてはその時点以前のデータから特徴量を算出し，その月末時点で継続の有無を示す教師信号を付与する．一方，Max-date以降のデータは教師信号の生成の際のみに使用する．これにより，モデルが将来の情報を事前に学習してしまうことを防止できる．
さらに，最適化された報酬関数を用いて各特徴量に対する報酬を算出し，この報酬値にシグモイド関数を適用して0\text{〜}1の範囲に正規化することで，モデルの予測結果としての継続確率を得る．この確率Pが，次のレビュー依頼にレビュアーが受け入れるか否かの予測値として用いられる．


% \begin{figure*}[t]
%     \centering
% \includegraphics[width=1.0\textwidth]{./Hashimoto_fig/heatmap.pdf}
%     \caption{各モデルのクロス評価）}
%     \label{fig:result}
% \end{figure*}

\subsection{評価指標}
 % 評価指標にはPression,Recall,F1スコア，AUC-ROC(ROC 曲線下面積)，AUC-PR(Precision-Recall 曲線下面積)の５つを用いる
 % AUC-ROCでは，閾値に依存しない全体的な分類性能を評価し，継続者と離脱者を区別するモデルの基本的な能力を図る．さらに，本研究が用いるデータは，継続者（正例）と離脱者（負例）の差が顕著であり不均衡なクラスになっているため，不均衡なデータに対して予測性能をより適切に評価できることが知られているAUC-PRを使用する．本研究ではこのAUC-ROCの値を重視する．

本研究では，4つの評価指標（適合率，再現率，F1値，AUC-ROC)を用いる．AUC-ROCは閾値に依存せず，レビュー依頼を受け入れるレビュアーと反応しないレビュアーを区別するモデルの基本的な能力を総合的に評価する．

\subsection{クロス評価}
% 従来研究では，将来の任意の時点以降の活動の有無を予測していたが，本研究では，将来の任意の区間での活動の有無の予測を行い，この区間をスライドさせて予測を行う．また学習時のラベル付に置いても，同様に区間をスライドさせて評価行う．
% 具体的には，学習時のラベル付の区間をスライドさせた４パターン（0-3m, 3-6m, 6-9m, 9-12m）のモデルで，予測区間をスライドさせた４パターン（0-3m, 3-6m, 6-9m, 9-12m)の16パターンで評価を行う．


従来研究では，将来の任意の時点（例えば，開発者がOSSプロジェクトに参加して1年後）以降の活動の有無を予測していた．本研究では，将来の任意の区間での活動の有無を予測し，この区間をスライドさせながら予測することで，どの学習期間が将来の活動の予測に最も寄与するかを明らかにする．また，各学習期間において，報酬を算出するために参照する活動区間も，同様にスライドする．具体的には，レビュー依頼を受けた時点の月の末日を基準に，学習時に参照する活動区間をスライドさせた4パターン（0\text{〜}3か月，3\text{〜}6か月，6\text{〜}9か月，9\text{〜}12か月）のモデルと，予測区間をスライドさせた4パターン（0\text{〜}3か月，3\text{〜}6か月，6\text{〜}9か月，9\text{〜}12か月）を組み合わせた計16パターンで評価を行う．

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{ケーススタディ}
\todo{色々やる}
\label{sec:casestudy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{データセット}
% \subsubsection{対象プロジェクト}
本研究では，仮想マシンインスタンスのプロビジョニングとライフサイクル管理を行う，OpenStackの中核的なコンピュートサービスOpenStack/Novaを対象とした．表\ref{table:dataset}は，分析対象プロジェクトの統計データを示す．

% \subsubsection{実験設計}
\begin{table}[h]
    \centering
    \caption{データセットの概要}
    \begin{tabularx}{\columnwidth}{XX}
        \hline
        項目 & 値 \\
        \hline
        対象期間 & 2021-01-01 ～ 2024-01-01 \\
        学習期間 & 2021-01-01 ～ 2023-01-01 \\
        予測期間 & 2023-01-01 ～ 2024-01-01 \\
        \hline
        総レビュー依頼数 & 約5,473件 \\
        訓練時レビュアー数 & 約70人 \\
        予測時レビュアー数 & 約60人 \\ 
        \hline
    \end{tabularx}
    \label{table:dataset}
\end{table}


\section{RQ1:逆強化学習に基づく提案モデルは，コードレビューにおけるタスク受け入れをどの程度予測できるか？}

%--------------------
% \begin{table}[h]
%     \label{table:train}
%     \centering
%     \caption{3-6mモデルの学習期間内における貢献予測結果}
%     \begin{tabularx}{\columnwidth}{XXXXX}
%         \hline
%        Precision & Recall & F1値 & AUC-ROC \\
%        \hline
%        0.909  & 0.556 & 0.689 & 0.818 \\ 
%        \hline
%     \end{tabularx}
% \end{table}
% %--------------------

% %--------------------
% \begin{table}[h]
%     \label{table:predict}
%     \centering
%     \caption{3-6mモデルで3-6m後の貢献予測結果}
%     \begin{tabularx}{\columnwidth}{XXXXX}
%         \hline
%        Precision & Recall & F1値 & AUC-ROC \\
%        \hline
%        0.769  & 0.556 & 0.645 & 0.820 \\ 
%        \hline
%     \end{tabularx}
% \end{table}
%--------------------
%--------------------
\begin{figure}[ht]
    \centering
\includegraphics[width=0.7\textwidth]{./Hashimoto_fig/RQ1.pdf}
    \caption{訓練・予測期間における予測結果}
    \label{fig:result1}
\end{figure}
%--------------------

RQ1では，構築したIRLのモデルが訓練対象の期間における開発の行動パターンを適切に学習し妥当な予測を行うことができるのか，また過学習になっていないかを検証することを目的としている．
図\ref{fig:result1}は，3-6ヶ月後に貢献があるかをラベル付し学習した3-6mのモデルにおける，学習期間での予測の精度と，予測期間での予測の精度を示している．具体的には，Precision, recall, F1 Score, AUC-ROCの4つの評価指標を用いており，各指標のグラフの左側が訓練期間，右側が予測期間の結果を示している.\\
　Precision（適合率）は継続と予測したもののうち実際に継続した割合，Recall（再現率）は実際に継続したもののうち正しく継続と予測できた割合，F1-scoreはPrecisionとRecallの調和平均，AUC-ROCは，レビュー承諾/レビュー拒否の二値分類性能を総合的に評価した指標である．
この結果から学習期間内で評価を行なった値と，評価期間で評価を行なった値が同様の結果を示す．これは，提案モデルが学習データに過剰に適合しておらず，未知の期間に対しても一定の汎化性を持つことを示唆している．
また，評価期間の予測におけるAUC-ROCが0.820であり，0.500よりも高いため，提案モデルがレビュー依頼を承諾する貢献者と離脱者を効果的に区別できることを示している．

\section{RQ2:学習方法や予測期間の長さに応じて，予測モデルの精度はどのように変化するか?}
%--------------------
\begin{figure}[t]
    \centering
\includegraphics[width=0.8\textwidth]{./Hashimoto_fig/IRLheatmap.pdf}
    \caption{IRLの各モデルのクロス評価}
    \label{fig:IRLheatmap}
\end{figure}
%--------------------

%--------------------
\begin{figure}[t]
    \centering
\includegraphics[width=0.8\textwidth]{./Hashimoto_fig/RFheatmap.pdf}
    \caption{RFの各モデルのクロス評価}
    \label{fig:RFheatmap}
\end{figure}
%--------------------
% 図4のヒートマップは縦軸を予測する区間でスライドさせたものであり，横軸は学習時に付与するラベルの指定期間をスライドさせたものである．それぞれのモデルでスライドさせた予測期間を5つの評価指標で評価し．ヒートマップを作成した結果である．学習時のラベル付の期間と予測期間が一致する際に予測精度が最も高くなることが期待されていたが，結果として，全てのモデルにおいて，予測期間3-6mでAUC-PRが最大になった．他の予測期間においても，全てのモデルで同様の結果が見られ，AUC-PRが高いものから順に並べると3-6m, 0-3m, 9-12m, 6-9mの順になった．

RQ2では，学習期間から予測期間までの時間的距離がモデル性能に与える影響についてクロス評価を分析する．図\ref{fig:IRLheatmap}は，学習期間と予測期間の組み合わせによる精度の違いを，4つの評価指標を用いて評価した結果をヒートマップで示している．横軸は訓練期間，縦軸は評価期間を示す．ラベル付の期間と予測期間が一致する際に予測精度が最も高くなることが期待されたが，9-12m以外のモデルで訓練期間より後の区間（0-3mの場合は3-6m,6-9mなど）でAUC-ROCが高くなる傾向が見られ，期待された結果は得らなかった．3-6mのモデルで全体的にAUC-ROCが高く，9-12mのモデルではAUC-ROCが低下することがわかった．
さらに，比較対象として多くの既存手法で用いられているRF(Random Forest)を採用し，同様の評価を行なった．図\ref{fig:RFheatmap}は，RFを用いて4つの評価指標を用いて評価した結果を示す．
RFは一貫して精度が高く，AUC-ROCに着目すると，訓練期間と評価期間が一致する対角線上で,提案手法であるIRLより値が高くなっていることがわかる．一方で，IRLは訓練期間と評価期間が異なる期間で，高い値になっていることがわかる．特に，訓練期間が3-6mのモデルに着目すると，どの評価期間に関してもRFより値が高くなっていることがわかる．



\section{RQ3:推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か？}
RQ3では，提案モデル（IRL）および比較モデル（RF）を用いて特徴量の重要度分析を行い，継続的なタスク受諾に寄与する要因を明らかにする．
比較手法であるRFモデルの重要度分析に関しては,Gini係数に基づく重要度の分析を行なった．これは，決定木の不純度の減少量を基準としており，その特徴量が分類精度向上にどの程度寄与するのかを示す指標である．
提案手法である，IRLではニューラルネットワークによって近似された報酬関数 $R(s,a)$ を解析対象とする．ここでは，入力する特徴量の変化が継続確率である，報酬値に与える影響を分析するために，勾配ベースの重要度分析を行なった．具体的には，学習済みモデルにおいて入力特徴量に対する報酬関数の偏微分係数（勾配）を算出する。これは、ある特徴量の値を微小に変化させた際、出力である報酬値がどの程度変動するかを定量化するものであるこれは，例えばある特徴量の入力値を僅かに増加させた結果，出力される報酬値が増加すればその特徴量は継続を促進する，正の影響を与えていると解釈できる．逆に，入力の増加に伴って報酬値が減少すればそれは継続を阻害する，負の影響を与えていることになる．


図5は報酬関数における，状態特徴量と行動特徴量の各特徴量の重要度をそれぞれモデル別で分析したものである．図5からは，レビュアーの貢献を予測する状態特徴量として総レビュー数が，行動特徴量としては協力度が強い正の影響を与えていることが明らかになった．
また，平均活動間隔やレビュー規模は強い負の影響を与えていることが明らかになった．


\begin{figure}[ht]
    \centering
\includegraphics[width=1.0\textwidth]{./Hashimoto_fig/importance.pdf}
    \caption{各特徴量の予測における重要度}
    \label{fig:importance}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{考察}
\section{時系列データの考慮が予測に与える影響}
\todo{やる}
これは分析対象とするnovaプロジェクトが3ヶ月ごとにリリースしていることも一要因として考えられる．したがって，短期間の活動パターンから将来のレビュー依頼に対して開発者が受け入れるか否かを予測できることが示唆される．しかし
\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{短期予測と長期予測における特徴量の違い}
，
図\ref{fig:importance}に示す通り,初期段階では総コミット数などの活動量がモデルに寄与している一方，その値は徐々に低下し，協力度や応答速度などの特徴量が強く寄与するようになる．これは，0-3mのモデルで，予測区間が9-12mにまで離れると予測精度が低下する原因になると考えられる．
RQ3では，総コミット数やレビュー数などの活動量が予測に寄与したが，期間が進むにつれてその影響が減少し，協力度やコード品質といった活動量以外の要因の寄与が高まった．レビュー規模は一貫して負の影響を示し，レビュー負荷がかかり過ぎると離脱要因になることが示唆される．これらの結果から，短期間の予測では活動量による影響が強く，長期間になるにつれて協力度や応答速度といった行動の量ではなく質的な要因の影響が高くなると考える．

\chapter{妥当性の脅威}

\section{内的妥当性の脅威}
\todo{}
\section{外的妥当性の脅威}
\todo{}

本研究では，OpenStack/Novaという単一プロジェクトを対象としている．OpenStack/Novaは独自の開発形態やレビュープロセスを持っており，本研究で得られた結果が，他のドメインや開発プロセスを持つプロジェクト（例：GitHubを使用しているプロジェクトなど）においては，今回の結果とは異なる新たな知見が得られる可能性がある．しかし，OpenStack/Novaは，大規模かつ長期間運用されている代表的なプロジェクトであるため，OSS開発における継続的な貢献パターンの一般的プロジェクトの一例として有効な知見が得られると考える．

また，継続の予測に用いた状態・行動特徴量がレビュアーの継続要因の全てを満たしているわけではない．しかし，レビュー活動の代表的な質的・量的な特徴量を定義することで，OSSレビュアーの多様な行動の傾向を理解するための一指標として有効であると考える．

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{おわりに}
\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
本研究では，コードレビュープロセスを対象に，長期的貢献者の予測を行うモデルを提案した．従来では将来の単一時点での貢献の有無を予測していたのに対し，本手法は逆強化学習を用いることで活動履歴を時系列で捉え，レビュアーの行動を説明する報酬関数を推定し，将来の単一時点ではなく，連続的な区間での貢献の有無を予測することを試みた．

今後の課題としては，妥当性の脅威に対処していくために，GitHubを利用しているプロジェクトなどを対象とした追加分析を行うこと，本研究では考慮しなかったタスクの専門性の一致度などといった特徴量を追加し予測精度の向上を目指す．さらに本研究では単一プロジェクトを対象にしていたが，複数プロジェクトを対象に分析を行うことで，プロジェクト間でのレビュアーの動きを捉えることを目指す．


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% 謝辞
%%
%% \begin{acknowledgements}
%% 感謝します．
%% \end{acknowledgements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% 参考文献
%%
\bibliographystyle{junsrt}
\bibliography{@Bachelor2025_Hashimoto/references}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% 付録
%%
% \appendix
% 
% \chapter{サンプルプログラム}
% 
% プログラムリストや実行結果など，本論を補足する上で必要と思われるものが
% あれば付録として付ける．
% 
% {
% \footnotesize
% \begin{verbatim}
% #include <stdio.h>
% int main(void)
% {
%     printf("Hello, World!\n");
%     return 0;
% }
% \end{verbatim}
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
