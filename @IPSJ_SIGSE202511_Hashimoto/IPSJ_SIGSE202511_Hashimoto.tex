%%
%% 研究報告用スイッチ
%% [techrep]
%%
%% 欧文表記無しのスイッチ(etitle,eabstractは任意)
%% [noauthor]
%%

%\documentclass[submit,techrep]{ipsj}
\documentclass[submit,techrep,noauthor]{ipsj}



% \usepackage[dvips]{graphicx}
\usepackage{latexsym}
\usepackage[dvipdfmx]{graphicx}
\usepackage{tabularx}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{amsmath}

\newcommand{\todo}[1]{\colorbox{yellow}{{\bf TODO}:}{\color{red} {\textbf{[#1]}}}}
\newcommand{\ihara}[1]{\colorbox{green}{{\bf IHARA}:}{\color{blue} {\textbf{[#1]}}}}
\newcommand{\memo}[1]{\colorbox{magenta}{\textbf{MEMO}}{\color{red}\textbf{[#1]}}}

\def\Underline{\setbox0\hbox\bgroup\let\\\endUnderline}
\def\endUnderline{\vphantom{y}\egroup\smash{\underline{\box0}}\\}
\def\|{\verb|}
%

%\setcounter{巻数}{59}%vol59=2018
%\setcounter{号数}{10}
%\setcounter{page}{1}


\begin{document}


\title{コードレビューにおける\\長期貢献者予測に向けた学習期間の検討}

\affiliate{WU}{和歌山大学\\
Wakayama University, 930 Sakaedani, Wakayama 640--8510, Japan}

\author{橋本 一輝}{Hashimoto Kazuki}{IPSJ}[s276189@wakayama-u.ac.jp]
\author{伊原 彰紀}{Ihara Akinori}{IPSJ}[ihara@wakayama-u.ac.jp]


\begin{abstract}
オープンソースソフトウェアプロジェクトでは，長期的に貢献する開発者（長期貢献者）が継続的な開発・保守において重要な役割を担っている．長期貢献者を早期に特定するためには，プロジェクト参加後の数ヶ月間の貢献の観察を要する．\todo{本研究では，それをなんとかするって言いたい}
本研究は，\todo{できれば参加後の数ヶ月を必要としない．．．みたいなこと言いたい}コードレビュー依頼を継続的に引き受ける長期貢献者を特定する手法を提案する．具体的には，逆強化学習を用いてレビュアーの貢献を学習し，継続的なコードレビュー依頼の受け入れ可否を決定する報酬関数を推定するモデルを構築する．ケーススタディとして，\todo{XXプロジェクトにおけるXXを対象とした結果，XXが明らかとなった．}
% オープンソースソフトウェアプロジェクトの継続的な開発・保守の実現には，長期的に貢献する開発者（長期貢献者）の確保が不可欠であるが，多くの開発者は数回の貢献後に活動が途絶えてしまうことが多い.従来研究では，長期貢献者を早期に特定する研究が進められており，プロジェクト参加後の数ヶ月の貢献に基づいて行われてきたが、コードレビュー作業への継続的な貢献を対象にした研究は進められていない.本研究では、コードレビュー依頼において，逆強化学習（IRL）を用いてレビュアーの貢献を学習し、継続的なタスク受け入れの判断を決定する報酬関数を推定する。
% 特に、レビュアーがタスクを受け入れる参加確率モデルの予測性能を検証するため、モデルの学習期間と貢献予測期間を動的に変更し、その予測精度と安定性を系統的に調査する。ケーススタディとしてコードレビュー履歴を対象に評価実験を行う．
% 本研究では，レビュアーの貢献が時間とともにどのように変化し、長期貢献者の予測に最適な学習期間を定量的に分析する。
\end{abstract}


%
\begin{jkeyword}
\todo{情報処理学会論文誌ジャーナル，\LaTeX，スタイルファイル，べからず集}
\end{jkeyword}
%
%\begin{eabstract}
%This document is a guide to prepare a draft for submitting to IPSJ
%Journal, and the final camera-ready manuscript of a paper to appear in
%IPSJ Journal, using {\LaTeX} and special style files.  Since this
%document itself is produced with the style files, it will help you to
%refer its source file which is distributed with the style files.
%\end{eabstract}
%
%\begin{ekeyword}
%IPSJ Journal, \LaTeX, style files, ``Dos and Dont's'' list
%\end{ekeyword}

\maketitle

%1
\section{はじめに}
オープンソースソフトウェア (OSS) は，地理的に分散した開発者によって実装，保守されているが，多くの開発者は，OSS開発組織から金銭的な利益を得ることなく，個人の知的好奇心や開発技術の学習などを動機として貢献している\cite{motivation}.
%論文誌：OSS コミュニティにおけるオープンコラボレーションの理解
Raymondらは，OSS開発の成功例としてLinuxを対象に，このように不特定多数の開発者が自発的に参加する開発形態をバザール方式と名付けている\cite{bazaar}．この開発モデルは，多様な専門知識を有する開発者の参加を促進する一方で，開発者の流動性が極めて高い~\todo{引用}．
開発者の動機が多様なため，時間的制約や興味の変化によって数回の貢献後に活動を停止する開発者が多い\cite{OTC}．こうした開発者の離脱は，OSSプロジェクトの失敗の一要因になっている\cite{failed}

% オープンソースソフトウェア (OSS) は，世界中のボランティア開発者の貢献によって発展している．これらの貢献者は，営利的な目的ではなく，社会的・技術的など多様な動機に基づき，違いに知識を共有しながら開発を進めてきた．この開発モデルはRaymondが提唱した「伽藍とバザール」における「バザール」モデルに相当する\cite{bazaar}．この開発モデルでは，開発者の多様な参加を促進する一方で，開発者の流動性が極めて高いという課題を抱えている


OSSを長期的に運用，保守しているプロジェクトでは，長期的に貢献する長期的貢献者 (LTC) が重要な役割を担っている\cite{related1}．
% 書籍：K. Fogel, Producing open source software: How to run successful free software project, Sebastopol, CA, O'Reilly Media, 2005.
\todo{LTCの定義をここで書いておきたい}
LTCは，ソフトウェアの大部分を実装しているプロジェクトも存在する\todo{引用}．
%Evidence for the Pareto principle in Open Source Software Activity (Goeminne & Mens, 2011)
%Revisiting the applicability of the Pareto principle to core teams in open source software projects (Yamashita et al., 2015)
また，実装に限らず，新規参加した開発者のメンター\todo{引用}，コードレビュー\todo{引用}などを行い，一部のLTCには，リポジトリの主要なブランチにコミットする権限が与えられ，プロジェクトの継続的な運用を支えている．このような経験が豊富なLTCは，流動性の高い組織構造を有するOSSプロジェクトに少数しか存在しないため，LTCに作業負担がかかることが少なくない\cite{related2}．LTCへの負担軽減のためにも，継続的に貢献する開発者の参加が喫緊の課題である\todo{引用}．


% プロジェクトを持続可能性を確保する上で重要なのが，プロジェクトに長期的に貢献する長期的貢献者(LTC)の存在である．LTCは単に新しいコードを作成するだけでなく，新規開発者の育成や，ソフトウェアコードレビューといったプロジェクトの中核的な役割を担う．コード作業やレビュープロセスには十分な知識を要するため，経験が豊富なLTCに作業が集中しやすい\cite{related2}．これらの問題を緩和するために，将来のLTC候補となる貢献者を特定し，候補者が離脱しないようサポートを行うことが重要である．



従来研究では，将来的にLTCとして長期間にわたってプロジェクトに参加する開発者を早期に特定する手法が提案されている\todo{引用}．\todo{XXらは}が開発したLTC候補者予測モデルは，プロジェクト参加後の1間月間の貢献を観察し\todo{XX}などのメトリクスを用いることで\todo{XX程度の精度で}LTCを特定することを実現している．しかし，LTCが担うタスクの1つであるコードレビュー作業への継続的な貢献に着目した研究は十分に進められていない．\todo{ではなくて，できれば参加後の数ヶ月を必要としない．．．みたいなこと言いたい}．本研究は，開発者が作業依頼の受け入れ，または依頼に反応しない記録を明確に取得可能なコードレビューを題材に，将来のコードレビュー依頼に対するレビュアーの貢献行動可否を，開発者の継続的な貢献と捉えたLTC候補者予測モデルを提案する．\todo{ここで，「受け入れは，開発者の興味，つまり継続的に貢献する意思がある，反応なしは継続的に貢献する意思がないことを意味する」みたいな，ことを書きたい．}具体的な予測モデルの構築には，\todo{（強化学習ではなく逆強化学習である理由）}のため，逆強化学習 (IRL: Inverse Reinforcement Learning) を用いる．本手法は，レビュアーがコードレビュー依頼を受け入れる際の判断指標\todo{判断指標？確率のこと？}（報酬関数）を推定し，継続的に貢献するレビュアーを予測するモデルを構築する．特に，モデルの学習方法，予測期間を変更することにより，予測精度の遷移を調査することで，長期貢献者\todo{長期貢献者は予測しているのか？}の予測モデルの検討を行う．本手法を評価するために次の3つの Reseach Questions（RQs）に回答する．
\begin{itemize}
    \item RQ1:逆強化学習に基づく提案モデルは，コードレビューにおける長期貢献者\todo{長期貢献者なのか？}をどの程度予測できるか？
    \item RQ2:学習時のラベル\todo{ラベルとは？}の付け方・予測期間の長さに応じて，予測モデルの精度はどのように変化するか?
    \item RQ3:推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か?
\end{itemize}


続く\ref{sec:related}章では，本研究の関連研究を紹介し，本研究の位置付けを明確にし，\ref{sec:ml}章では，本研究で用いる，逆強化学習とそれに関連する強化学習を紹介し，\ref{sec:model}章では，本研究の提案手法を述べ，\ref{sec:casestudy}章でケーススタディの結果を述べRQに回答する．\ref{sec:discussion}で本研究の結果を考察し，\ref{sec:conclusion}章で本研究をまとめる．




%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%3
\section{関連研究}\label{sec:related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{OSSへ継続的に貢献する開発者の研究}

従来研究では，開発者が特定のプロジェクトで活動を継続する動機を調査している．Wuらによる定性的分析において，OSS開発者は利他的な動機（例: 貢献による名誉や実績）や人的資本の向上に関連する経済的動機（例：専門知識を有する開発者との協調）をもってOSSプロジェクトへの参加を継続していることを明らかにしている\todo{引用}．
%Wu CG, Gerlach Jh, Young CE (2007) An empirical analysis of open source software developers’ m0otivation and continuance intentions. Informatnion and Management 44(3):153–262
その他，多くの研究者がOSSプロジェクトにおける新規参加者の定着に影響を与える要因を調査してい\todo{引用4本}．
%Bird C, Gourley A, Devanbu P, Swaminathan A, Hsu G (2007) Open borders? immigration in open source projects. In: Proceedings of the 4th International Workshop on Mining Software Repositories (MSR’07), pp 6–13
%Jensen C, Scacchi W (2007) Role migration and advancement processes in OSSD projects: a comparative case study. In: Proceedings of the International Conference on Software Engineering (ICSE’07), pp 364–374
% Steinmacher I, Chaves AP, Gerosa MA (2013) Why do newcomers abandon open source software projects? In: Proceedings of the InrernationalWorkshop on Cooperative and Human Aspects of Software Engineering (CHASE’13), pp 25–32
%Zhou M, Mockus A (2012) What make long term contributors: willingness and opportunity in oss community. In: Proceedings of the 34th International Conference on Software Engineering (ICSE’12), pp 518–528

Birdらは，開発者がOSSプロジェクトに貢献する期間に影響を与える要因を理解するためのハザード率モデル（hazard-rate model）を提案している\todo{引用}．
%%Bird C, Gourley A, Devanbu P, Swaminathan A, Hsu G (2007) Open borders? immigration in open source projects. In: Proceedings of the 4th International Workshop on Mining Software Repositories (MSR’07), pp 6–13
ケーススタディとして，Apache，Python，PostgreSQLプロジェクトでは，特定のプロジェクトで約1年間活動した開発者をリポジトリの主要なブランチにコミットする権限を与えていることを示している．特に，ApacheおよびPythonプロジェクトでは，継続的にパッチを作成している開発者に権限を与えられていることを明らかにしている．このように，OSSプロジェクトは，LTC候補者に継続的な貢献を期待していることが確認できる．


\subsection{長期貢献者候補者の研究}

従来研究では，LTC候補者を予測する手法が提案されている．\todo{引用}
%Zhou M, Mockus A (2012) What make long term contributors: willingness and opportunity in oss community. In: Proceedings of the 34th International Conference on Software Engineering (ICSE’12), pp 518–528
Zhouらは，OSSプロジェクトに新規に参加した開発者の中からLTC候補者を特定するモデルを開発している\cite{related1}\todo{引用}．当該研究において，LTC候補者に分類される開発者は，技術的能力，意欲，および環境に依存することを明らかにしている．特に，LTC候補者を特定するためには，プロジェクト参加後の1間月間の貢献を観察し\todo{XX}などのメトリクスを用いることで\todo{XX程度の精度で}LTCを特定することを実現している．
%%Zhou M, Mockus A (2012) What make long term contributors: willingness and opportunity in oss community. In: Proceedings of the 34th International Conference on Software Engineering (ICSE’12), pp 518–528



ELuriらは，OSSプロジェクトに新規に貢献した開発者の特徴量，および貢献したリポジトリの特徴量に基づき，開発者が3年後に貢献しているか否かを予測するモデルを開発している\cite{related1}．提案モデルは，従来手法の予測精度に比べて非常に高く，AUCが0.913という結果を得ている．当該研究の比較対象とされたBaoらの研究も同様に，T年後 (T=1 ,2, 3) に開発者が貢献しているか否かを，プロジェクト参加直後1ヶ月間の貢献から予測モデルを構築している．これらの研究は，OSSの開発記録であるコミットやイシューへの対応など，プロジェクトでの全般的な活動を貢献とみなし，将来の単一時点での予測に留まっている．\todo{なぜ単一時点だとだめなの？橋本研究ではどうなの？}

% 彼らが比較対象としたBaoらの研究\cite{related2}も同様に，T年後(T=1 ,2, 3)に貢献者が存在しているかを，プロジェクト参加時点から1ヶ月間の活動から予測する研究であった．

% 者がプロジェクトに参加した時点で収集が可能な31個の特徴量（コントリビュータに関する特徴量18個，リポジトリの特徴量13個)を用いて，貢献者が３年後も活動があるかどうかを予測するモデルを作成した．予測には５つの機械学習モデルが使用され，その結果Ramdom Forestが最も優れた性能を示し，AUCは0.913であった．彼らが比較対象としたBaoらの研究\cite{related2}も同様に，T年後(T=1 ,2, 3)に貢献者が存在しているかを，プロジェクト参加時点から1ヶ月間の活動から予測する研究であった．
% これらの研究は，commitやissue対応といったプロジェクトでの全般的な活動を貢献とみなしている．また，予測においても将来の単一時点での予測に留まっている．

\subsection{開発者の離職予測に関する研究}

\todo{これって，前の節に混ぜてもいいのでは？}
従来研究は，開発者の離脱を予測する手法を提案している．Linらは，プロジェクトに参加直後の6ヶ月間の月次レポート\todo{月次レポートって何？}に基づき，1年後に離職しているか否かを予測するモデルを提案している\todo{引用}．評価実験の結果，ELuriらの研究と同様にRamdom Forestが最も優れた性能を示し，\todo{Random Forestのことは書かなくていいので消す予定ですが，RandomForestの精度がこんなに良いって書くなら，RandomForest使えばいいのに，って気になるw}．予測に寄与する特徴量には，月次レポートのタスク報告内容，労働時間の標準偏差，プロジェクト開発者での労働時間\todo{OSSじゃないの？どうやって計測したんやろ？}の標準偏差であった．この研究も同様に，月次レポートに基づく活動でありプロジェクトでの全般的な活動を貢献とみなしている．また，予測に関しても単一の時点である．

\subsection{本研究の位置付け}
%2.1及び2.2の
従来研究では，コミットや月次レポートなどのプロジェクトへの全ての貢献量に基づき，特定の期間以降の開発者の自主的な貢献の有無を予測する手法を提案している．OSS開発では，自主的に貢献する活動に限らず，開発者間の協調作業として作業依頼に基づく活動も少なくない．本研究では，他の開発者からの依頼，特にコードレビュー依頼を受け入れるか否かを予測し，開発者の継続的な貢献と捉えたLTC候補者予測モデルを提案する．具体的には，開発者がOSSに貢献した期間\todo{時期とかくべき？}に応じて，0$\sim$3ヶ月後，3$\sim$6ヶ月後，6$\sim$9ヶ月後にそれぞれ貢献の有無を予測する．

OSS開発者の中には，一時的にプロジェクトでの活動を休止することが少なくない．そのため，従来研究では開発者の継続的な活動期間を計測することが難しかった．本研究で提案するモデルは\todo{XX}を用いるため，開発者の個人的な都合による一時的に休止を理解することは困難であるが，特に作業なく活動を休止している場合に，作業依頼を受け入れるか否かを予測できるモデルが構築できると考える．

% 単一時点での予測を行っており，LTCや開発者の定着性を予測する上で重要な知見を提供している．
% ここで，本研究ではプロジェクトでの全般的な活動ではなく，コードレビュー作業というより専門的な活動に焦点を当て分析する．
% また，予測においてもT年後という単一時点の予測から0-3ヶ月後，3-6ヶ月後，6-9ヶ月といった連続的な区間における貢献の有無を予測する．これにより，従来の研究では捉えることができなかった，貢献者が「いつ」活動し，「いつ」離脱するのかといった動的なパターンを捉えることを可能にすることを目的とする．具体的には．また，一度活動を休止した後に再び貢献するといったパターン（例:0-3ヶ月後は活動，3-6ヶ月後は休止,6-9ヶ月後に活動）を特定できる可能性がある．これにより，プロジェクトの管理者が，開発者が「いつ」離脱する可能性が高くなるか，あるいは復帰する可能性があるかを予測することができる．

%4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:ml}
\section{強化学習と逆強化学習}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

本研究では，LTC候補者予測モデルを構築するために，開発者の\todo{XX}を学習するために逆強化学習を，\todo{LTC候補者を予測するために？}強化学習を用いる．\memo{今回強化学習は使っていません．逆強化学習の報酬関数に基づき確率を算出して予測しています}

% 逆強化学習を使用する．以下で強化学習と合わせて説明する．
\subsection{強化学習}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{./Hashimoto_fig/RL.pdf}
    \caption{強化学習のイメージ図\todo{この図に，コードレビュー依頼に置き換えた図を合わせて載せては？4章で使う用語の説明にも使えそう.}}
    \label{fig:RL}
\end{figure}


強化学習 (RL: Reinforcement Learning) は，エージェントと環境が相互作用を通じて，累積報酬を最大化するような行動の方策を学習する機械学習の一分野である．\todo{表Xは}以下の基本的な構成要素から成る．\todo{以下の箇条書き，表にして，3列目にコードレビュー依頼に置き換えた説明を追記しては？}
\begin{itemize}
    \item エージェント：学習・意思決定を行う主体
    \item 環境：エージェントと相互作用する対象
    \item 状態：環境の現在状況（スナップショット）
    \item 行動：エージェントが取ることの選択肢
    \item 報酬：行動の良し悪しに応じて与えられるスカラー量
    \item 方策：エージェントの行動を決定する
\end{itemize}

強化学習において，報酬関数$R(s, a)$は最も重要な要素の一つであり，エージェントにとって何が望ましい行動であるかを示す指標となる．しかし，多くの実世界の問題では，多様な要因が複雑に作用しているため，事前に報酬関数を設計することは困難であり，不適切な報酬関数はエージェントを意図しない行動に導くことになる．コードレビュー依頼でも同様であり，\todo{...XX...}

\subsection{逆強化学習}
強化学習において事前に報酬関数を設計することが困難である課題を解決するため，専門家の行動から，その行動を説明する報酬関数を逆算するアプローチとして逆強化学習が用いられる\todo{引用}．既知の報酬関数から最適な方策を学習する強化学習に対して，逆強化学習は既知の専門家の軌跡から，その行動を説明する報酬関数を推定する．専門家の軌跡は，状態と行動を対として時系列に並べたものであり，専門家はある報酬関数（目的）を最大化するよう行動していると仮定し，その報酬関数を推定する．

従来の教師あり学習でのLTC予測では，単一時点でのスナップショットから予測を行うモデルを作成し，予測を行っていたため，過去の文脈\todo{コードレビュー依頼における文脈とは？}の変化のパターンを捉えることが難しい．しかし，逆強化学習は，時系列の活動を軌跡として扱い，各状態と行動から報酬関数を推定するため，過去の行動パターン\todo{この研究では具体的に何になる？}が将来の行動の予測に影響を与える．そのため従来では捉えることが難しかった，行動パターンが継続につながるのかを，過去の文脈に基づきより正確に捉えることができる．


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%5
\section{長期貢献者予測モデル}
\label{sec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{データ収集}
\subsubsection{データソース}
本研究では，Gerrit REST APIを用いて，gerritからOpenStackプロジェクトのデータを取得する．

\subsubsection{収集データ\todo{これは表にして，4.1と4.2を統合する}}
\begin{itemize}
    \item レビュー依頼(Change)
    \begin{itemize}
        \item 変更ID,プロジェクト名,作成日時,ステータス,コード変更量(追加/削除)
    \end{itemize}
\end{itemize}
\begin{itemize}
    \item レビュアー情報
    \begin{itemize}
        \item メールアドレス，名前，アサイン日時
    \end{itemize}
\end{itemize}
\begin{itemize}
    \item レビュー活動
    \begin{itemize}
        \item  レビューコメント内容，投票スコア，応答時刻
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item 活動履歴
    \begin{itemize}
        \item  過去活動統計(レビュー回数，レビュー応答率，プロジェクト在籍日数)
    \end{itemize}
\end{itemize}
\begin{itemize}
    \item レビュアーとの相互用
    \begin{itemize}
        \item レビュアーと変更作成者間の過去の相互作用回数？？
    \end{itemize}
\end{itemize}

\subsection{データ前処理}
収集したデータから自動化されたbotアカウント(CI/CD等)と判断されるものを\todo{どうやって？}除外する．全てのデータをrequest-time(レビュー依頼時刻)でソートし，時系列順に整列する．\todo{訓練用データと評価ようデータの区別はここで説明では？}



\subsection{ラベル付\todo{ラベルって言葉は強化学習であまり使わないんじゃないかな？ここのタイトルは「評価値の決定」とか？}}
\subsubsection{報酬関数}
\begin{figure}[t]
    \centering
    \includegraphics[width = 0.5\textwidth]{./Hashimoto_fig/label.pdf}
    \caption{図:指定期間が0-3mであった時のラベル付の例}
    \label{fig:label}
\end{figure}


本研究では，強化学習によりLTC候補者予測モデルを構築するために，逆強化学習を用いて報酬関数を推定する．ここで用いる逆強化学習では，コードレビュー依頼時点から指定期間内に貢献が生じた場合をTrue，生じなかった場合をFalseとする二値変数として表現し，これを学習の入力データとする．


% \subsubsection{ラベルの基準点の設定}
% レビュアーの活動ごとにラベルを付与すると，ラベルの数が膨大になり，学習時間の増加や，細かすぎるラベル付により類似した状態での学習が増加するため，過学習になる恐れがある．そのため，本研究では各月の末日を基準点とすることで，その月内の全活動に同一のラベルを付与する，これにより月単位での継続パターンを学習し，より一般化された予測モデルの構築を目指す．

\subsubsection{基準点の設定}
レビュアーの活動ごとに評価値を割り当てると，その数が膨大になり，学習時間の増加や，類似する状態に基づく学習が増加することで，過学習を引き起こすおそれがある．そこで，本研究では各月の末日を基準点とし，その月内の全活動に同一の評価値を割り当てる．本手法により，月単位での継続的な活動パターンを学習し，より一般化が可能な\todo{一般化の意味がよくわからんけど，一旦スルー}予測モデルの構築を目指す．

\subsubsection{ラベル付の手順\todo{「評価値決定の手順」とか？}}
評価値の決定は次の手順で行う．
% 実際のラベル付は次の通りに行う．
\begin{enumerate}
    \item 活動日から活動月を特定する
    \begin{itemize}
        \item 例:2022-01-05 → 2022年1月
    \end{itemize}
    \item その月の最終日を基準点とする
    \begin{itemize}
        \item 例：2022-01-31
    \end{itemize}
    \item 基準点から指定期間の間で将来の活動があるか調べる（指定期間が0-3mの場合）
    \begin{itemize}
        \item 例：2022-01-31から2022-04-31に活動があるか（基準日から指定期間）
    \end{itemize}
    \item レビュー依頼を受け入れるか否かを判定
    \begin{itemize}
        \item 活動あり→True, レビュー拒否→False
    \end{itemize}
        \item その月の全活動に同じラベルを付与する（4）でTrueの場合
    \begin{itemize}
        \item 1月の全活動にTrueラベルを付与
    \end{itemize}
\end{enumerate}

% \subsubsection{ラベルの学習時の役割}
% このラベルは，逆強化学習の際に単なる過去の事実としてではなく，その時点での状態が，将来の継続とどう関連しているかを学習するための教師信号として機能する．具体的には，Trueのラベルからは，継続したレビュアーの活動から継続を促進する要因を学習し，Falseのラベルからは離脱したレビュアーのパターンから離脱のシグナルを学習することができる．

\subsubsection{学習時における指標値の役割}
本研究で用いる指標値は，逆強化学習において単なる過去の事実を示すもの†ではなく，レビュー依頼時の状態が将来の継続にどのように関係するかを学習するための信号として機能する．具体的には，評価値がTrueの指標値からはレビュアーが貢献を継続するする要因を学習し，評価値がFalseの指標値からはレビュアーが依頼に反応しない行動パターンを示す特徴を学習する．


\subsection{状態と行動の特徴量抽出}
本研究では，逆強化学習を行うにあたり，9つの状態特徴量（表\ref{table:stateList}）と，4つの行動特徴量（表\label{table:actionList}）を使用する．
%---------------
\begin{table}[t]
    \centering
    \label{table:stateList}
    \caption{状態特徴量（9次元）\todo{特徴量名は日本語にする}}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{特徴量名} & \textbf{定義} \\
        \midrule
        \texttt{experience\_days} & 初回活動から現在までの日数 \\
        \texttt{total\_changes} & これまでの総コミット数 \\
        \texttt{total\_reviews} & これまでのレビュー数 \\
        \texttt{project\_count} & 参加したプロジェクト数 \\
        \texttt{recent\_activity\_frequency} & 30日間での活動数 \\
        \texttt{avg\_activity\_gap} & タイムスタンプ間の平均日数 \\
        \texttt{activity\_trend} & 月単位での活動量の差\\
        \texttt{collaboration\_score} & その開発者にレビューした数 \\ 
        \texttt{code\_quality\_score} & test/doc/refactor/fixの頻度 \\
        \bottomrule
    \end{tabular}

\vspace{2mm}

    \centering
    \caption{行動特徴量（4次元）}
        \begin{tabular}{@{}ll@{}}
        \toprule
        \label{table:actionList}
        \textbf{特徴量名} & \textbf{定義} \\
        \midrule
        \texttt{intensity} & レビュータスクの変更行数 \\
        \texttt{message\_quality} & レビューリクエストのコメント内容 \\
        \texttt{collavoration} & パッチ作成者に対するレビュー依頼の応答率 \\
        \texttt{latency} & レビューリクエストから応答までの日数 \\
        \bottomrule
    \end{tabular}
\end{table}
%---------------



\subsection{学習：損失関数と報酬関数の推定}
本研究のモデル学習は，主に目的関数である損失関数の設定と逆強化学習 (IRL\todo{この英略いる？}) の原理に基づく報酬関数の最適化のプロセスから構成する．

\subsubsection{損失関数}
% 貢献者の予測というタスクにおいて，約８割の開発者が長期貢献者になる前に，離脱してしまうということが知られている[todo]，そのため，正例と負例の間で学習データ数に不均衡が生じる．この不均衡問題を対処するために，本研究では，Focal Lossを使用する．

従来研究では多くの開発者が長期貢献者と呼ばれるまでに離脱する[todo]，そのため，LTC候補者予測のデータセットは正例（LTC候補者）と負例（LTC候補者以外の開発者）の間で学習データ数に不均衡が生じる．この不均衡問題を対処するために，本研究では，損失関数を使用する．

\subsubsection{損失関数の定義}

本研究では，損失関数としてFocal Lossを用いる．Focal lossは，二値クロスエントロピー (BCE: Binary Cross-Entropy) を拡張した損失関数の一種である．二値クロスエントロピーは，二値分類のモデルの予測確率と正解ラベルの差を評価する損失関数であり，正解に近い予測ほど損失が小さく，誤った予測ほど損失が大きくなる．二値クロスエントロピーは次の式で示せる．
\[\textbf{二値クロスエントロピー：Binary Cross-Entropy (BCE)}\]
\[
L_{\mathrm{BCE}} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i\log(p_i) + (1 - y_i)\log(1 - p_i)\right]
\]
ここで，$N$はデータセットのサンプル数，$y_i \in \{0, 1\}$は$y_i$の正解ラベル，モデルが予測したクラスの確率は$p_i \in [0, 1]$を示す．

\[p_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}\]はシグモイド関数を表す．しかし，二値クロスエントロピーは，クラス不均衡が生じた際に，容易に分類できるサンプルばかりが学習され，難しいサンプルの学習が十分に進まない問題を解決するように設計されている．具体的には，正しく予測できる容易なサンプルの損失の重みを小さくし，誤分類されやすい難しいサンプルに学習の焦点が当たるように調整する．Focal Lossは次の式で示せる．

% Focal Loss(FL)は，二値クロスエントロピー（BCE）において，クラスの不均衡が生じた際に，容易に分類できる学習ばかりが進み，分類が困難なサンプルの学習ができない問題を解決するために設計された．具体的には，分類が容易である（正しく予測することができる）サンプルの損失の重みを小さくし，難しいサンプルに焦点が当たるように設計されている
% \[\textbf{二値クロスエントロピー：Binary Cross-Entropy (BCE)}\]
% \[
% L_{\mathrm{BCE}} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i\log(p_i) + (1 - y_i)\log(1 - p_i)\right]
% \]
% ここで，$N$はデータセットのサンプル数，$y_i \in \{0, 1\}$は$y_i$の正解ラベル，モデルが予測したクラスの確率は$p_i \in [0, 1]$を示す．

% \[p_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}\]はシグモイド関数を表す．


\[\textbf{Focal Loss (FL)}\]
\begin{align}
L_{\mathrm{FL}} = - \frac{1}{N} \sum_{i=1}^{N} \Bigl( & \alpha (1 - p_i)^{\gamma} y_i \log(p_i) \nonumber \\
& + (1 - \alpha) p_i^{\gamma} (1 - y_i) \log(1 - p_i) \Bigr) \nonumber 
\end{align}


ここで，
\(\alpha \in [0,1]\) はクラス不均衡を補正する重みパラメータを示し，
\(\gamma \ge 0\) は難しいサンプルを強調する焦点パラメータである．  
\(\gamma = 0\) のとき，Focal Loss は BCE に一致する．


\subsubsection{報酬関数}
\begin{figure*}[t]
    \centering
    \includegraphics[width = 1.0\textwidth]{./Hashimoto_fig/prediction.pdf}
    \caption{学習から予測のフロー}
    \label{fig:RL}
\end{figure*}

% 逆強化学習では専門家の行動を説明する報酬関数を学習する．具体的には，状態sで行動aを選択することの価値を表す報酬関数R(s,a；θ)を，ニューラルネットワーぅのパラメータθとして学習する．本研究での専門家は継続したレビュアーを指し，非専門家は離脱したレビュアーを指す．学習においては，専門家（継続者）の活動軌跡に対しては高い累積報酬を与え，非専門家（離脱者）の活動軌跡に対しては低い累積報酬を与えるように，報酬関数R(s,a;θ)を調整する．


逆強化学習では，専門家の行動を説明する報酬関数を学習する．本研究ではレビュアーがレビュー依頼を受け入れるか，あるいは反応しないかを指す．具体的には，レビュアーが状態$s$において，レビュー依頼を受け入れる，または反応しないという行動$a$を選択する価値を表す報酬関数$R(s, a; \theta)$を，ニューラルネットワークのパラメータ$\theta$として学習する．学習において，継続してレビュー依頼を受け入れるレビュアーの活動軌跡に対しては高い累積報酬を与え\todo{この表現あってる？}，依頼に反応しないレビュアーの活動軌跡に対しては低い累積報酬を与えるように，報酬関数$R(s, a; \theta)$を調整する．


\subsubsection{モデル構造と学習フロー}
本研究で学習に用いるニューラルネットワークは，エンコーダ，LSTM，デコーダから構成される．

\noindent\textbf{エンコーダ}：レビュアーの各活動を表す状態 (State) および行動 (Action) の2つの特徴量ベクトルを，それぞれ独立したエンコーダ（多層パーセプトロン)に入力する．これらのエンコーダは元のベクトルよりもより表現力のある高次元のベクトルに変換する役割を持つ．

\noindent\textbf{LSTM}：各時点$t$\todo{tってどこで使ってる？数式や図もないのでtと置く必要ない？}でエンコードされた状態ベクトルと行動ベクトルを連結し，時系列プロセッサであるLSTM (Long Short-Term Memory) に入力する．LSTMは活動履歴を記憶し，時間的な依存関係を理解するため，単一の活動だけでなく，活動量が徐々に低下する動的なパターンも把握できる．

\noindent\textbf{デコーダ}：LSTMによりレビュアーの全活動が処理された後，LSTMで得られた時系列情報（特徴ベクトル）をもとに多層パーセプトロンで最終的な予測値を算出する．ここではLSTMで処理されたレビュアーの全活動履歴を考慮した情報をデーコードし，シグモイド関数により最終的な継続確率$P$\todo{Pってどこで使ってる？数式や図もないのでtと置く必要ない？}を算出する．



\subsubsection{Focal Lossによる報酬関数の最適化}

継続確率$P$はシグモイド関数$\sigma$を通して，軌跡の累積報酬todo\todo{??}に関連付けられる．累積報酬が高いほど継続確率$P$は高くなる．学習時には4.3節で定義した評価値を用い，予測確率Pと評価値の誤差を計算するためにFocal Lossを用いる．この誤差が最小となるように，最適化アルゴリズム (Adam Optimizer) で反復的に学習を行うことにより，継続して貢献するレビュアーの行動パターンを高く評価し，離脱者のパターンを低く評価する報酬関数を作成する．


\subsection{予測・評価}
% 本研究では，学習時に予測時点以降の情報を使用することを防ぐために，学習期間と予測期間を分離するだけでなく，特徴量計算に使用できるデータの最大時点 (Max-date) を設けている．
% 具体的には，各時点においてはその時点以前のデータから特徴量を計算し，その月末の時点で継続ラベルを付与するが，Max-date以降のデータはラベル生成の際のみ使用する．これにより，モデルが将来の情報を事前に学習することを防ぐことができる．
% ここで，最適化された報酬関数を用いて，特徴量に対する報酬を算出する．この報酬値にシグモイド関数を適応することで0\textasciitilde1の継続確率に変換することで予測を行う．

本研究では，学習時に予測時点以降の情報を使用することを防ぐため，学習期間と予測期間を分離するだけでなく，特徴量計算に使用可能なデータの最大時点（Max-date）を設定している．具体的には，各時点においてはその時点以前のデータから特徴量を算出し，その月末時点で継続の有無を示す教師信号を付与する．一方，Max-date以降のデータは教師信号の生成の際のみに使用する．これにより，モデルが将来の情報を事前に学習してしまうことを防止できる．
さらに，最適化された報酬関数を用いて各特徴量に対する報酬を算出し，この報酬値にシグモイド関数を適用して0\text{〜}1の範囲に正規化することで，モデルの予測結果としての継続確率を得る．この確率Pが，次のレビュー依頼にレビュアーが受け入れるか否かの予測値として用いられる．


% \begin{figure*}[t]
%     \centering
% \includegraphics[width=1.0\textwidth]{./Hashimoto_fig/heatmap.pdf}
%     \caption{各モデルのクロス評価）}
%     \label{fig:result}
% \end{figure*}

\ihara{ここまで修正済み}

\subsubsection{評価指標}
 % 評価指標にはPression,Recall,F1スコア，AUC-ROC(ROC 曲線下面積)，AUC-PR(Precision-Recall 曲線下面積)の５つを用いる
 % AUC-ROCでは，閾値に依存しない全体的な分類性能を評価し，継続者と離脱者を区別するモデルの基本的な能力を図る．さらに，本研究が用いるデータは，継続者（正例）と離脱者（負例）の差が顕著であり不均衡なクラスになっているため，不均衡なデータに対して予測性能をより適切に評価できることが知られているAUC-PRを使用する．本研究ではこのAUC-ROCの値を重視する．

本研究では，4つの評価指標（適合率，再現率，F1値，AUC-ROC（ROC曲線下面積）を用いる．AUC-ROCは閾値に依存せず，レビュー依頼を受け入れるレビュアーと反応しないレビュアーを区別するモデルの基本的な能力を総合的に評価する．

\subsubsection{クロス評価}
% 従来研究では，将来の任意の時点以降の活動の有無を予測していたが，本研究では，将来の任意の区間での活動の有無の予測を行い，この区間をスライドさせて予測を行う．また学習時のラベル付に置いても，同様に区間をスライドさせて評価行う．
% 具体的には，学習時のラベル付の区間をスライドさせた４パターン（0-3m, 3-6m, 6-9m, 9-12m）のモデルで，予測区間をスライドさせた４パターン（0-3m, 3-6m, 6-9m, 9-12m)の16パターンで評価を行う．


従来研究では，将来の任意の時点（例えば，開発者がOSSプロジェクトに参加して1年後）以降の活動の有無を予測していた．本研究では，将来の任意の区間での活動の有無を予測し，この区間をスライドさせながら予測することで，どの学習期間が将来の活動の予測に最も寄与するかを明らかにする．また，各学習期間において，報酬を算出するために参照する活動区間も，同様にスライドする．具体的には，，レビュー依頼を受けた時点を基準に，\todo{$\leftarrow$この説明であってる？}学習時に参照する活動区間をスライドさせた4パターン（0\text{〜}3か月，3\text{〜}6か月，6\text{〜}9か月，9\text{〜}12か月）のモデルと，予測区間をスライドさせた4パターン（0\text{〜}3か月，3\text{〜}6か月，6\text{〜}9か月，9\text{〜}12か月）を組み合わせた計16パターンで評価を行う．\todo{本当に16で評価するのか，ヒートマップ右下はいらんかも．}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ケーススタディ}
\label{sec:casestudy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{データセット}
\subsubsection{対象プロジェクト}
本研究では，仮想マシンインスタンスのプロビジョニングとライフサイクル管理を行う，OpenStackの中核的なコンピュートサービスOpenStack/Novaを対象とした．
\subsubsection{実験設計}
\begin{table}[h]
    \centering
    \caption{データセットの概要}
    \begin{tabularx}{\columnwidth}{XX}
        \hline
        項目 & 値 \\
        \hline
        対象期間 & 2021-01-01 ～ 2024-01-01 \\
        学習期間 & 2021-01-01 ～ 2023-01-01 \\
        予測期間 & 2023-01-01 ～ 2024-01-01 \\
        \hline
        総レビュー依頼数 & 約10,908件 \\
        訓練時レビュアー数 & 241人 \\
        予測時レビュアー数 & 167人 \\
        \hline
    \end{tabularx}
    \label{table:dataset}
\end{table}




\subsection{RQ1:逆強化学習に基づく提案モデルは，コードレビューにおける長期貢献者をどの程度予測できるか？}

%--------------------
\begin{table}[h]
    \centering
    \caption{3-6mモデルの学習期間内における貢献予測結果}
    \begin{tabularx}{\columnwidth}{XXXXX}
        \hline
       Precision & Recall & F1 & AUC-ROC \\
       \hline
       0.909  & 0.556 & 0.689 & 0.818 \\ 
       \hline
    \end{tabularx}
\end{table}
%--------------------

%--------------------
\begin{table}[h]
    \centering
    \caption{3-6mモデルで3-6m後の貢献予測結果}
    \begin{tabularx}{\columnwidth}{XXXXX}
        \hline
       Precision & Recall & F1 & AUC-ROC \\
       \hline
       0.769  & 0.556 & 0.645 & 0.820 \\ 
       \hline
    \end{tabularx}
\end{table}
%--------------------

表4は学習で使用したデータでどの程度予測を行うことができるのかを検証した結果であり,表5は評価期間において，3-6mのモデルで3-6m後の予測を行なった結果である．表5，6から学習期間内で評価を行なった値と評価期間で評価を行なった値が同様の結果になっており，評価期間の予測におけるAUC-ROCが0.820である．このことから提案モデルがレビュアーの継続者と離脱者を効果的に区別できることを示している．

\subsection{RQ2:学習時のラベルの付け方・予測期間の長さに応じて，予測モデルの精度はどのように変化するか?}
%--------------------
\begin{figure*}[t]
    \centering
\includegraphics[width=0.7\textwidth]{./Hashimoto_fig/heatmap.pdf}
    \caption{各モデルのクロス評価}
    \label{fig:result}
\end{figure*}
%--------------------
% 図4のヒートマップは縦軸を予測する区間でスライドさせたものであり，横軸は学習時に付与するラベルの指定期間をスライドさせたものである．それぞれのモデルでスライドさせた予測期間を5つの評価指標で評価し．ヒートマップを作成した結果である．学習時のラベル付の期間と予測期間が一致する際に予測精度が最も高くなることが期待されていたが，結果として，全てのモデルにおいて，予測期間3-6mでAUC-PRが最大になった．他の予測期間においても，全てのモデルで同様の結果が見られ，AUC-PRが高いものから順に並べると3-6m, 0-3m, 9-12m, 6-9mの順になった．

図4のヒートマップは縦軸が予測する区間でスライドさせたものであり，横軸は学習時に付与するラベルの指定期間をスライドさせ，4つの評価指標で評価を行なった結果である．ラベル付の期間と予測期間が一致する際に予測精度が最も高くなることが期待されたが，9-12m以外のモデルで訓練期間より後の区間（0-3mの場合は3-6m,6-9mなど）でAUC-ROCが高くなる傾向が見られ，期待された結果は得らなかった．3-6mのモデルで全体的にAUC-ROCが高く，9-12mのモデルではAUC-ROCが低下する．

\subsection{RQ3:推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か？}
図5は報酬関数における，状態特徴量と行動特徴量の各特徴量の重要度をそれぞれモデル別で分析したものである．図5からは，レビュアーの貢献を予測する状態特徴量として総レビュー数が，行動特徴量としては協力度が強い正の影響を与えていることが明らかになった．
また，平均活動間隔やレビュー規模は強い負の影響を与えていることが明らかになった．


\begin{figure}[h]
    \centering
\includegraphics[width=0.5\textwidth]{./Hashimoto_fig/importance.pdf}
    \caption{各特徴量の予測における重要度}
    \label{fig:importance}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{考察}
\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{考察}
RQ2において，訓練期間に対して，訓練期間以降での予測期間で評価したAUC-ROCが高かったことから3ヶ月間の時間差では行動パターンが大きく変化しないことが考えられる．このため短期間の活動パターンからある程度の先の予測区間まで予測を行えることが考えられる．しかし，図５に示す通り各特徴量の重みが初期段階では総コミット数といった活動量が予測に重要な影響を与えていたが，徐々にその割合が低下し，協力度や応答速度といった特徴が重要となってくるため，0-3mのモデルで，予測区間が9-12mにまで離れてしまうと予測精度が低下する原因になると考えられる．\\
　また，AUC-ROCに関して3-6mモデルが最も高い値を示した．この要因としては，3-6ヶ月以降の期間ではレビュアーの行動パターンが安定化し，レビュー承諾者とレビュー依頼を受けない貢献者（離脱者）の行動のパターンの差が明確になるため，効率的に識別できたことが考えられる．さらに，オープンスタックのリリースサイクルは６ヶ月であるため，モデルで学習した行動パターンが次のサイクルでも再現されやすく，精度が高まったことが考えられる．\\
　RQ3においては，総コミット数やレビュー数などの活動量が予測に大きく寄与したが，期間が進むにつれてその影響が減少し，協力度やコード品質といった活動量以外の要因の寄与が高まった．レビュー規模はい一貫して負の影響を示し，レビュー負荷がかかり過ぎると離脱要因となる可能性が示唆された．これらの結果から，短期間の予測では活動量による影響が強く，長期間になるにつれて協力度や応答速度とった行動の量ではなく質的な要因の影響が高くなることが示唆された．

\subsection{妥当性の脅威}
本研究では，OpenStack/Novaという単一プロジェクトを対象としている．OpenStack/Novaは独自の開発形態やレビュープロセスを持っており，本研究で得られた結果が，他のドメインや開発プロセスを持つプロジェクト(例：GitHubを使用しているプロジェクなど)においては，今回の結果とは異なる新たな知見が得られる可能性がある．しかし，OpenStack/Novaは，大規模かつ長期間運用されている代表的なプロジェクトであるため，OSS開発における継続的な貢献パターンの一例として
また，0-3m,3-6m...３ヶ月単位で指定した将来の区間における活動の有無という二値分類を行なったが，この3ヶ月という区切り方が適切でない可能性がある．また，継続の予測に用いた状態・行動特徴量がレビュアーの継続要因の全てを満たしているわけではない．

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{おわりに}
\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
本研究では，コードレビュープロセスを対象に，長期的貢献者の予測を行うモデルを提案した．従来では将来の単一時点での貢献の有無を予測していたのに対し，本手法は逆強化学習を用いることで活動履歴を時系列で捉え，レビュアーの行動を説明する報酬関数を推定し，将来の単一時点ではなく，連続的な区間での貢献の有無を予測することを試みた．
% Novaプロジェクトを対象にケーススタディを行ったところ，ランダム予測を大きく上回，コードレビューにおける長期貢献者を効果的に予測できる可能性を示した．また，予測期間が3-6m後の未来の区間である際に最も予測性能が高くなること傾向が見られた，さらに，推定された報酬関数をレビュアーの継続には最近の活動頻度が正に，平均活動間隔が負に大きく寄与するといったことが示唆された．\\
 今後の課題としては，妥当性の脅威を対処していくために，GitHubを利用しているプロジェクトなどを対象とした追加分析を行うこと，本研究では考慮しなかったタスクの専門性の一致度などといった特徴量を追加し予測精度の向上を目指す．さらに本研究では単一プロジェクトを対象にしていたが，複数プロジェクトを対象に分析を行うことで，プロジェクト間でのレビュアーの動きを捉えることを目指す．


\bibliographystyle{ipsjunsrt}
\bibliography{references}


\end{document}

